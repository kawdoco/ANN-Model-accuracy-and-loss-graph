{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-16-3070bbee1457>, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-3070bbee1457>\"\u001b[1;36m, line \u001b[1;32m28\u001b[0m\n\u001b[1;33m    print(\"CIFAR-10 data already downloaded.\")\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import zipfile\n",
    "import sys\n",
    "\n",
    "try: \n",
    "    from urllib.request import urlretrieve \n",
    "except ImportError:\n",
    "    from urllib import urlretrieve \n",
    "\n",
    "def _download_progress(count, block_size, total_size):\n",
    "    percent_complete = float(count * block_size) / total_size\n",
    "    msg = \"\\r- Data download progress: {0:.1%}\".format(percent_complete)\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def download_and_extract():\n",
    "    _dir = \"./\"\n",
    "    data_dir = \"./cifar-10-batches-py\"\n",
    "\n",
    "    if os.path.exists(data_dir):\n",
    "    print(\"CIFAR-10 data already downloaded.\")\n",
    "        return\n",
    "    else:\n",
    "        url = \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "        filename = url.split('/')[-1]\n",
    "        file_path = os.path.join(_dir, filename)\n",
    "        zip_cifar_10 = file_path\n",
    "        file_path, _ = urlretrieve(url=url, filename=file_path, reporthook=_download_progress)\n",
    "\n",
    "        print()\n",
    "        print(\"Download finished. Extracting files in \" + data_dir + \".\")\n",
    "        if file_path.endswith(\".zip\"):\n",
    "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(_dir)\n",
    "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
    "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(_dir)\n",
    "        print(\"Done.\")\n",
    "\n",
    "        os.remove(zip_cifar_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def unpickle( file ):\n",
    "\ttry: \n",
    "\t\timport cPickle\n",
    "\t\tfo = open(file, 'rb')\n",
    "\t\tdict = cPickle.load(fo)\n",
    "\texcept ImportError:\n",
    "\t\timport _pickle as cPickle\n",
    "\t\tfo = open(file, 'rb')\n",
    "\t\tdict = cPickle.load(fo, encoding='latin-1')\n",
    "\n",
    "\tfo.close()\n",
    "\treturn dict\n",
    "\n",
    "def one_hot_vec(label):\n",
    "\tvec = np.zeros(10)\n",
    "\tvec[label] = 1\n",
    "\treturn vec\n",
    "\n",
    "def whiten_data(features):\n",
    "\tfeatures = (features - np.mean(features, axis=0)) / np.std(features, axis=0)\n",
    "\treturn features\n",
    "\n",
    "def load_data(ratio = 0.8):\n",
    "\tfor _ in range(5):\n",
    "\t\td = unpickle( 'cifar-10-batches-py/data_batch_' + str(_+1) )\n",
    "\n",
    "\t\tif(_ == 0):\n",
    "\t\t\t_x = d['data']\n",
    "\t\t\t_y = d['labels']\n",
    "\t\telse:\n",
    "\t\t\t_x = np.vstack((_x, d['data']))\n",
    "\t\t\t_y = np.concatenate((_y, d['labels']), axis=0)\n",
    "\n",
    "\td = unpickle('cifar-10-batches-py/test_batch')\n",
    "\t_x = np.vstack((_x, d['data']))\n",
    "\t_y = np.concatenate((_y, d['labels']), axis=0)\n",
    "\t\n",
    "\t_x = whiten_data(_x)\n",
    "\t_y = np.array(list(map(one_hot_vec, _y)))\n",
    "\n",
    "\tindex = int(ratio * len(_x)) # Split index\n",
    "\tx_train = _x[0:index, :]\n",
    "\ty_train = _y[0:index]\n",
    "\tx_test = _x[index:,:]\n",
    "\ty_test = _y[index:]\n",
    "\n",
    "\tprint(\"Data Split: \", ratio)\n",
    "\tprint(\"Train => x:\", x_train.shape, \" y:\", len(y_train))\n",
    "\tprint(\"Test  => x:\", x_test.shape, \" y:\", len(y_test))\n",
    "\n",
    "\treturn [x_train, y_train, x_test, y_test]\n",
    "\n",
    "class DataLoader():\n",
    "\tdef __init__(self):\n",
    "\t\tdownload_and_extract()\n",
    "\t\tself.x_train, self.y_train, self.x_test, self.y_test = load_data()\n",
    "\n",
    "\tdef next_batch(self, batch_size):\n",
    "\t\tlength = self.x_train.shape[0]\n",
    "\t\tindices = np.random.randint(0, length, batch_size) # Grab batch_size values randomly\n",
    "\t\treturn [self.x_train[indices], self.y_train[indices]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Data download progress: 100.0%\n",
      "Download finished. Extracting files in ./cifar-10-batches-py.\n",
      "Done.\n",
      "Data Split:  0.8\n",
      "Train => x: (48000, 3072)  y: 48000\n",
      "Test  => x: (12000, 3072)  y: 12000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-489fb434d631>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mNUM_H2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_INPUTS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_OUTPUTS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "cifar = DataLoader()\n",
    "\n",
    "learning_rate = 0.0001\n",
    "num_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "_WIDTH = 32; _HEIGHT = 32; _CHANNELS = 3 \n",
    "NUM_INPUTS = _WIDTH * _HEIGHT * _CHANNELS \n",
    "NUM_OUTPUTS = 10\n",
    "NUM_H1 = 512\n",
    "NUM_H2 = 256\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, NUM_INPUTS])\n",
    "Y = tf.placeholder(tf.float32, [None, NUM_OUTPUTS])\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()  \n",
    "fc1 = tf.layers.dense(X, NUM_H1, activation=tf.nn.relu, kernel_initializer=he_init, name='fc1') \n",
    "fc2 = tf.layers.dense(fc1, NUM_H2, activation=tf.nn.relu, kernel_initializer=he_init, name='fc2') \n",
    "logits = tf.layers.dense(fc2, NUM_OUTPUTS, name='logits')\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "trainer = optimizer.minimize(loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "_step = []\n",
    "_acc = []\n",
    "for step in range(num_steps):\n",
    "    batch_xs, batch_ys = cifar.next_batch(batch_size)\n",
    "    sess.run( trainer, feed_dict={X: batch_xs, Y: batch_ys} )\n",
    "\n",
    "    if(step % display_step == 0):\n",
    "      acc = sess.run(accuracy, feed_dict={X: cifar.x_test, Y: cifar.y_test }) \n",
    "      _step.append(step)\n",
    "      _acc.append(acc)\n",
    "\n",
    "      print(\"Step: \" + str(step) + \" Test Accuracy: \" + str(acc)) \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
